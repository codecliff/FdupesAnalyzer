####################### Developer #######################################
# Copyright : Rahul Singh
# URL       : https://github.com/codecliff/FdupesAnalyzer
# License   : MIT 
# email     : 

####################### Description #####################################
# An R Script to parse output for fdupes utility for finding duplicate files 
# input file generated by command 
# fdupes -Sr .>> dupes_root_2020_01_28.txt 
# and copied to input directory
######################## Output ########################################
# Final Result is a csv file with these columns :
# "dir1"    : directory 1     
# "dir2"    : directory 2    
# "matchcnt": no. of files matching betweent dir1 and dir2 
# "acnt"    : file count in dir1     
# "bcnt"    : file count in dir2   
# "aprct"   : percent of files in dir1 whch have copy in dir2    
# "bprct"   : same for dir2 
# "maxprct" : max of above two 
########################################################################

# library("dplyr")
# install.packages('VennDiagram')
#library(VennDiagram)
library("tools") #only for extarcting filename, better use regex

library(data.table)


########################################################################
# Load and Parse Fdupes output 
# Convert to DataFrame
# Uses data.table for better memory management
# ~2.4GB Ramfor a file of 70,000 rows
########################################################################

#create output directory 
if(!dir.exists("./output")){
  dir.create("./output")
}

##################### User Input #######################################
#root direcotry given to fdupes ## (user specified)
pathRoot <- "/home/devel/sshfs/srv/dev_diskx"
# input file, generated by command 
# fdupes -Sr .>>dupes_results.txt
dat <- readLines("data/dupes_results.txt") ## (user specified)
########################################################################

head(dat, 10)

#remove empty lines
dat <- dat[dat != ""]
length(dat)

# rows separating file matches
delimiter_rows <-  which(grepl("bytes each:", dat))
length(delimiter_rows) 


# Converting raw file to dataframe
myDF <- data.table(
  groupID = integer(),
  fname = character(),
  type = character(),
  dir = character(),
  file = character(),
  stringsAsFactors = F
)
myDF
currFileID = 0
nrow(myDF)
for (i in 1:length(dat) ) {
#for (i in 1:18000 ) {
  col
          if (i %in% delimiter_rows) {currFileID = i}
          else{
            myDF<- rbind(myDF, list(currFileID , basename(dat[i]),
                                    tools::file_ext(dat[i]) ,
                                    dirname(dat[i]),  dat[i])
                         )
            # myDF[1 + nrow(myDF),] = list(currFileID , basename(dat[i]),
            #                              tools::file_ext(dat[i]) ,
            #                              dirname(dat[i]),  dat[i])
          }#else
  if(i%%100==0)print(i)
}#for

# View(myDF) #can comment this
dim(myDF)
head(myDF)

object.size(myDF)
object.size(as.data.frame(myDF) )
#Filter here if you only want files of certain type
# myDF<- myDF[myDF$type %in% c("pp3") ,]

write.csv(as.data.frame(myDF) , file= "output/myDF.csv", row.names = F)





########################################################################
# File count in each direcotry
########################################################################

# all unique direcotries with duplicate files
dirs <- unique(myDF$dir)

dirsDF <- data.frame(dir = dirs,
                     filecount = 0,
                     stringsAsFactors = F)
l=length(dirs) ; i=0;
for (d in dirs) {
  pth = sprintf("%s/%s", pathRoot, d)
  #print(pth)
  c1 = length(list.files(pth, all.files = T))-2 #this counts hidden but excludes .,..
  dirsDF$filecount[dirsDF$dir == d] = c1
  i=i+1
  print(sprintf("%s/%s", i,l))
}

dim(dirsDF)
head(dirsDF)
min(dirsDF$filecount)
# if you see any file counts as -2 , they are because 
# fdupes  traversed some   direcotrie
# for whch R does not have x permission 
sum(dirsDF$filecount<0)
head(dirsDF$dir[dirsDF$filecount<0])
dirsDF<- dirsDF[dirsDF$filecount>0, ]
dim(dirsDF)
write.csv(dirsDF, "output/dirsDF.csv", row.names = F)

dirsDF<- read.csv("output/dirsDF.csv", as.is = T)
sapply(X = dirsDF, FUN =  class)

#we keep only non-hidden directories 
dirs<- dirsDF$dir[dirsDF$filecount>0]
#length(dirs) == length(unique(dirs))
class(dirs)

########################################################################
# all possible unique pairs of directories
########################################################################


dirgps <-  t(combn(dirs, m = 2)) 
# combs does not return pairs to self, unlike expand.grid
# it will also not return reverse pairs [1,2 vs 2,1 ]

length(dirs) #1457
dim(dirgps) #1060696      2
#View(dirgps)



########################################################################
# Create A Large Datatable With All Comaprison Info
# This file is the  main output of this script 
# This will take some time but memory req < 1 GB 
########################################################################

#preallocated data tab;e 
n = nrow(myDF) 
compareDFAll <- data.table(
  dir1 = rep("",n),
  dir2 =  rep("",n),
  matchcnt =  rep(0,n),
  acnt = rep(0,n),
  bcnt = rep(0,n),
  aprct = rep(0,n),
  bprct = rep(0,n),
  stringsAsFactors = F
)
head(compareDFAll)
l=nrow(dirgps)
#l=16000
filled=0
for (i in 1:l) {
          
          d1 = dirgps[i, 1]
          d2 = dirgps[i, 2]
          #print(d1)
          #beware of double counting ! one dir can share same file with many
          g1 = unique(myDF$groupID[myDF$dir == d1])
          g2 = unique(myDF$groupID[myDF$dir == d2] )
          ovl <- sum(g1 %in% g2) #calculate.overlap(x= list(g1 , g2 ) )
          #if nothing common, move on
          if(ovl==0)next();
          
          if(filled%%10==0)
          print(sprintf("%s/%s :(%s filled)",i,l,filled) )
          filled= filled+1 
          acnt = dirsDF$filecount[dirsDF$dir == d1]
          bcnt = dirsDF$filecount[dirsDF$dir == d2]
          
          compareDFAll[filled, ] <- list(d1, d2,  ovl, acnt, bcnt,
                 round(100 * ovl / acnt), round(100 * ovl / bcnt))
           
          
  }#for

#retain only those rows which actually were filled 
compareDFAll<- compareDFAll[dir1!="",]
compareDFAll$maxprct <- apply(X = compareDFAll[, c("aprct", "bprct")], 1, max)
#sort
compareDFAll2 <-  compareDFAll[order(compareDFAll$maxprct, decreasing = T), ]

object.size(compareDFAll2)
dim(compareDFAll) 
head(compareDFAll2) #%>% View
tail(compareDFAll2)

#some sorting
setorderv(compareDFAll2, cols = c("matchcnt", "maxprct") , order =c(-1,-1) )

# save to file
#### This is the main outout of this cript. Open in spreadsheet and pore over it
write.csv(compareDFAll2, file = "output/compareDFAll.csv", row.names = FALSE)

View(compareDFAll2)
# compareDFAll<-compareDFAll2

#######################################################################################
# Extra Function : Which directory has most files common with this one ?
#######################################################################################
bestMatcherforDirectory <- function(mydir) {
  dgp = dirgps[dirgps[, 1] == mydir, ]
  # print(dgp)
  fctA = dirsDF$filecount[dirsDF$dir == mydir]
  
  compareDF2 <- data.frame(
                          dir1 = character(),
                          dir2 = character(),
                          matchcnt = integer(),
                          matchprct = integer(),
                          fcountA = integer(),
                          fcountB = integer(),
                          stringsAsFactors = F  )
  # compareDF2
  
  for (i in 1:nrow(dgp)) {
                          d2 = dgp[i, 2]
                          fctB = dirsDF$filecount[dirsDF$dir == d2]
                          g1 = myDF$groupID[myDF$dir == mydir]
                          g2 = myDF$groupID[myDF$dir == d2]
                          # ovl<-calculate.overlap(x= list(g1 , g2 ) )
                          ovl <- sum(g1 %in% g2) #length(ovl$a3)
                          prct <- 100 * ovl / fctA
                          compareDF2[1 + nrow(compareDF2), ] <-
                          list(mydir, d2, ovl, prct, fctA, fctB)
    
  }#for
  
  compareDF2 <- compareDF2[compareDF2$matchcnt > 0, ]
  compareDF2 <- compareDF2[order(compareDF2$matchcnt, decreasing = T), ]
  
  return(compareDF2)
  
}#bestMAtcherforDirectory

#bestMatcherforDirectory("./ntfs/2017-03-16_pictures_backup/2014_12_22")


