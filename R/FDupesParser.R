####################### Developer #######################################
# Copyright : Rahul Singh
# URL       : https://github.com/codecliff/FdupesAnalyzer
# License   : MIT 
# email     : 
# Disclaimer: No warranties, even for loss of data. Be careful when deleting files.   
####################### Description #####################################
# An R Script to parse output for fdupes utility for finding duplicate files 
# Also generates bash file for automating  an 'informed' duplicate removal

# input file generated by command 
# fdupes -Sr .>> dupes_root_2020_01_28.txt 

######################## Output ########################################
# Final Result is a csv file with these columns :
# "dir1"    : directory 1     
# "dir2"    : directory 2    
# "matchcnt": no. of files matching betweent dir1 and dir2 
# "acnt"    : file count in dir1     
# "bcnt"    : file count in dir2   
# "aprct"   : percent of files in dir1 whch have copy in dir2    
# "bprct"   : same for dir2 
# "maxprct" : max of above two 
########################################################################

# library("dplyr")
# install.packages('VennDiagram')
#library(VennDiagram)
library("tools") #TODO: only for extarcting filename, better use regex
library(data.table)
#library(dplyr)


########################################################################
# Load and Parse Fdupes output 
# Convert to DataFrame
# Uses data.table for better memory management
# ~2.4GB Ramfor a file of 70,000 rows
########################################################################

#create output directory 
if(!dir.exists("./output")){
  dir.create("./output")
}

##################### User Input #######################################
#root direcotry given to fdupes ## (user specified)
pathRoot <- "/home/devel/sshfs/srv/dev-disk-by-label-IMGext4/"
# input file, generated by command 
# fdupes -Sr .>>dupes_results.txt
dat <- readLines("data/fdupes_2.txt") ## (user specified)
########################################################################

head(dat, 10)

#remove empty lines
dat <- dat[dat != ""]
length(dat)

# rows separating file matches
delimiter_rows <-  which(grepl("bytes each:", dat))
length(delimiter_rows) 


# Converting raw file to dataframe
myDF <- data.table(
  groupID = integer(),
  fname = character(),
  type = character(),
  dir = character(),
  file = character(),
  stringsAsFactors = F
)
myDF
currFileID = 0
nrow(myDF)
for (i in 1:length(dat) ) {
#for (i in 1:18000 ) {
  col
          if (i %in% delimiter_rows) {currFileID = i}
          else{
            myDF<- rbind(myDF, list(currFileID , basename(dat[i]),
                                    tools::file_ext(dat[i]) ,
                                    dirname(dat[i]),  dat[i])
                         )
            # myDF[1 + nrow(myDF),] = list(currFileID , basename(dat[i]),
            #                              tools::file_ext(dat[i]) ,
            #                              dirname(dat[i]),  dat[i])
          }#else
  if(i%%100==0)print(i)
}#for

# View(myDF) #can comment this
dim(myDF)
head(myDF)

object.size(myDF)
object.size(as.data.frame(myDF) )

#TODO: Cannot Filter by  file extensions, fdupes limiation 
# myDF<- myDF[myDF$type %in% c("pp3") ,]

write.csv(as.data.frame(myDF) , file= "output/myDF.csv", row.names = F)





########################################################################
# File count in each direcotry
########################################################################

# all unique direcotries with duplicate files
dirs <- unique(myDF$dir)

dirsDF <- data.frame(dir = dirs,
                     filecount = 0,
                     stringsAsFactors = F)
l=length(dirs) ; i=0;
for (d in dirs) {
  pth = sprintf("%s/%s", pathRoot, d)
  #print(pth)
  c1 = length(list.files(pth, all.files = T))-2 #this counts hidden but excludes .,..
  dirsDF$filecount[dirsDF$dir == d] = c1
  i=i+1
  print(sprintf("%s/%s", i,l))
}

dim(dirsDF)
head(dirsDF)
min(dirsDF$filecount)

# if you see any file counts as -2 , they are because 
# fdupes  traversed some   direcotrie
# for whch R does not have x permission 
sum(dirsDF$filecount<0)
head(dirsDF$dir[dirsDF$filecount<0])

#if all counts are -2, the filesystem is nt readable from here
#we can still analyse 

noneRead= FALSE

if ( sum(dirsDF$filecoun == -2 ) == nrow(dirsDF) ){
  warning("Warning: rootpath cannot be read. Only match count info will be provided! ")
  dirsDF$filecount=0
}#if
head(dirsDF)

dirsDF<- dirsDF[dirsDF$filecount>(-2), ]
dim(dirsDF)
write.csv(dirsDF, "output/dirsDF.csv", row.names = F)

dirsDF<- read.csv("output/dirsDF.csv", as.is = T)
sapply(X = dirsDF, FUN =  class)
View(dirsDF)
#we keep only non-hidden directories 

dirs<- NULL

#filter for when we can access filesystem and not 
filtr<- ifelse( ( sum(dirsDF$filecoun == -2 ) == nrow(dirsDF) ) , -2, 0  )
filtr

dirs<- dirsDF$dir[dirsDF$filecount> filtr]
length(dirs)
#length(dirs) == length(unique(dirs))
class(dirs)

########################################################################
# all possible unique pairs of directories
########################################################################


dirgps <-  t(combn(dirs, m = 2)) 
# combs does not return pairs to self, unlike expand.grid
# it will also not return reverse pairs [1,2 vs 2,1 ]

length(dirs) #1457
dim(dirgps) #1060696      2
# View(dirgps)



########################################################################
# Create A Large Datatable With All Comaprison Info
# This file is the  main output of this script 
# This will take some time but memory req < 1 GB 
########################################################################

#preallocated data tab;e 
n = nrow(myDF) 
compareDFAll <- data.table(
  dir1 = rep("",n),
  dir2 =  rep("",n),
  matchcnt =  rep(0,n),
  acnt = rep(0,n),
  bcnt = rep(0,n),
  aprct = rep(0,n),
  bprct = rep(0,n),
  stringsAsFactors = F
)
head(compareDFAll)
l=nrow(dirgps)
#l=16000
filled=0
for (i in 1:l) {
          
          d1 = dirgps[i, 1]
          d2 = dirgps[i, 2]
          #print(d1)
          #beware of double counting ! one dir can share same file with many
          g1 = unique(myDF$groupID[myDF$dir == d1])
          g2 = unique(myDF$groupID[myDF$dir == d2] )
          ovl <- sum(g1 %in% g2) #calculate.overlap(x= list(g1 , g2 ) )
          #if nothing common, move on
          if(ovl==0)next();
          
          if(filled%%10==0)
          print(sprintf("%s/%s :(%s filled)",i,l,filled) )
          filled= filled+1 
          acnt = dirsDF$filecount[dirsDF$dir == d1]
          bcnt = dirsDF$filecount[dirsDF$dir == d2]
          
          compareDFAll[filled, ] <- list(d1, d2,  ovl, acnt, bcnt,
                 round(100 * ovl / acnt), round(100 * ovl / bcnt))
           
          
  }#for

#retain only those rows which actually were filled 
compareDFAll<- compareDFAll[dir1!="",]
compareDFAll<- compareDFAll[dir1!=dir2,] #make sure dir2 and dir2 are not same 

compareDFAll$maxprct <- apply(X = compareDFAll[, c("aprct", "bprct")], 1, max)
#sort
compareDFAll2 <-  compareDFAll[order(compareDFAll$maxprct, decreasing = T), ]

object.size(compareDFAll2)
dim(compareDFAll) 
head(compareDFAll2) #%>% View
tail(compareDFAll2)

#some sorting
setorderv(compareDFAll2, cols = c("matchcnt", "maxprct") , order =c(-1,-1) )

# save to file
#### This is the main outout of this cript. Open in spreadsheet and pore over it
write.csv(compareDFAll2, file = "output/compareDFAll.csv", row.names = FALSE)

View(compareDFAll2)

#######################################################################################
# Extra  :  Semi-automate file deletion , direcotry by direcotry 
#######################################################################################
## 1. (Preferably) Manualy Edit the compareDFAll.csv file, keeping this in mind: 
##    -Function Below will create fdupes command for each row
##    -First direcotry on each row will be conserved. Any dupes in 2nd directory will be deleted
##    -Make absolutely sure dir1 and dir2 are not same or it will be gone! (Though we have checked for this)
##
## 2. Load the edited File (command below)
## 3. Print out a long list of fdupes commands generated using each row 
## 4. Copy this list to a file
## 5. Run this text file using bash 
##
##
## We reuse fdupes to delete, ndwithout -r option. This is safer than manual delete.
## Files in first arg are perserved. 
## Don't use recursion as it will delete  from subfolders in first directory as well.
## you risk losing many auto-generated  files if you recurse

# load edited data 
compareddata<- read.csv("output/compareDFAll.csv", header = T,stringsAsFactors = F, as.is = T)
dim(compareddata)

#with sudo 
#sf<- function(i) { cat( sprintf('sudo fdupes -dN "%s" "%s"\n',compareddata$dir1[i], compareddata$dir2[i]) )}
#without sudo
sf<- function(i) {cat( sprintf('fdupes -dN "%s" "%s"\n',compareddata$dir1[i], compareddata$dir2[i]) ) }

for (i in 1:nrow(compareddata)) sf(i) # !! better alter this to run in small batches 


### Copy this output to text file and run  
### from roothpath directory 
# bash ./fduper.txt
print("bash ./fduper.txt" ,quote = FALSE)

### go through this text file very carefully before running.Make sure 
### the directry you want to preserve is first argument on the line


### command to get available space 
print( sprintf('df -Ph "%s"', pathRoot ) ,quote = FALSE )

### command to get directory size  
print ( sprintf('du -sh "%s"', compareddata$dir1[2] ) ,quote = FALSE  )

#######################################################################################
# Extra Function : Which directory has most files common with this one ?
#######################################################################################
bestMatcherforDirectory <- function(mydir) {
  dgp = dirgps[dirgps[, 1] == mydir, ]
  # print(dgp)
  fctA = dirsDF$filecount[dirsDF$dir == mydir]
  
  compareDF2 <- data.frame(
                          dir1 = character(),
                          dir2 = character(),
                          matchcnt = integer(),
                          matchprct = integer(),
                          fcountA = integer(),
                          fcountB = integer(),
                          stringsAsFactors = F  )
  # compareDF2
  
  for (i in 1:nrow(dgp)) {
                          d2 = dgp[i, 2]
                          fctB = dirsDF$filecount[dirsDF$dir == d2]
                          g1 = myDF$groupID[myDF$dir == mydir]
                          g2 = myDF$groupID[myDF$dir == d2]
                          # ovl<-calculate.overlap(x= list(g1 , g2 ) )
                          ovl <- sum(g1 %in% g2) #length(ovl$a3)
                          prct <- 100 * ovl / fctA
                          compareDF2[1 + nrow(compareDF2), ] <-  list(mydir, d2, ovl, prct, fctA, fctB)
    
  }#for
  
  compareDF2 <- compareDF2[compareDF2$matchcnt > 0, ]
  compareDF2 <- compareDF2[order(compareDF2$matchcnt, decreasing = T), ]
  
  return(compareDF2)
  
}#bestMAtcherforDirectory

# bestMatcherforDirectory("./IMAGES/Pictures_2017_oldlaptop_hdd")
